{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "import sklearn.feature_extraction \n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn.over_sampling\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/clb617/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clb617/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS & CONSTANTS\n",
    "#==============================================================================\n",
    "DATA_PATH = '/scratch/olympus/projects/lead_follow_state_legislation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Size: (59810, 4)\n",
      "Restricted Size: (59740, 4)\n"
     ]
    }
   ],
   "source": [
    "policy_tweets_ = pd.read_csv(\n",
    "    '{}/machine_learning/training_datasets/emily_ar_data_combined.csv'.format(DATA_PATH), \n",
    "    dtype = {\n",
    "        'text':str, \n",
    "        'political_bin':int, \n",
    "        'source':str,\n",
    "        'clean_text':str\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Original Size:\", policy_tweets_.shape)\n",
    "\n",
    "policy_tweets = policy_tweets_[policy_tweets_.text.str.len() > 20]\n",
    "policy_tweets.index = list(range(policy_tweets.shape[0]))\n",
    "print(\"Restricted Size:\", policy_tweets.shape)\n",
    "\n",
    "policy_tweet_texts = policy_tweets[\"text\"]\n",
    "policy_y_data = policy_tweets[\"political_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Size: (2623346, 9)\n"
     ]
    }
   ],
   "source": [
    "topic_tweets_ = pd.read_csv(\n",
    "    '{}/machine_learning/training_datasets/MEDIA-to-train-and-predict.csv'.format(DATA_PATH), \n",
    "    dtype = {\n",
    "        'clean_text_cnn':str, \n",
    "        'coder':str, \n",
    "        'dataset':str,\n",
    "        'tweet_id':str, \n",
    "        'text':str,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Original Size:\", topic_tweets_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_table = {\n",
    "    '1': 0, \n",
    "    '10': 9, \n",
    "    '12': 10, \n",
    "    '13': 11,\n",
    "    '14': 12, \n",
    "    '15': 13, \n",
    "    '16': 14, \n",
    "    '17': 15, \n",
    "    '18': 16,\n",
    "    '19': 17, \n",
    "    '2': 1, \n",
    "    '20': 18, \n",
    "    '21': 19, \n",
    "    '23': 20,\n",
    "    '3': 2, \n",
    "    '4': 3, \n",
    "    '5': 4, \n",
    "    '6': 5, \n",
    "    '7': 6, \n",
    "    '8': 7, \n",
    "    '9': 8\n",
    "}\n",
    "\n",
    "mt_list = [x for x in list(set([int(x) for x in list(cross_table.keys())]))]\n",
    "topic_labels = ['Economy', 'Civil Rights','Health','Agriculture',\n",
    "                'Labor and Employment', 'Education', 'Environment', \n",
    "                'Energy', 'Immigration', 'Transportation', 'Law and Crime',\n",
    "                'Social Welfare', 'Housing', 'Finance & Domestic Commerce',\n",
    "                'Defense', 'Science, Tech and Communications', 'Foreign Trade',\n",
    "                'International Affairs and Aid', 'Government Operations',\n",
    "                'Public Lands and Water', 'Arts and Entertainment']\n",
    "\n",
    "topic_label_index = dict(zip(mt_list, topic_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>majortopic01</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>68061</td>\n",
       "      <td>Government Operations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16.0</th>\n",
       "      <td>55569</td>\n",
       "      <td>Defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21.0</th>\n",
       "      <td>49245</td>\n",
       "      <td>Public Lands and Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>39191</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.0</th>\n",
       "      <td>33471</td>\n",
       "      <td>International Affairs and Aid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.0</th>\n",
       "      <td>32474</td>\n",
       "      <td>Law and Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <td>31535</td>\n",
       "      <td>Finance &amp; Domestic Commerce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>28497</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>25887</td>\n",
       "      <td>Transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.0</th>\n",
       "      <td>20186</td>\n",
       "      <td>Foreign Trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>19916</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>18991</td>\n",
       "      <td>Agriculture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>18403</td>\n",
       "      <td>Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>17743</td>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.0</th>\n",
       "      <td>17533</td>\n",
       "      <td>Social Welfare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>17456</td>\n",
       "      <td>Labor and Employment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>15884</td>\n",
       "      <td>Civil Rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.0</th>\n",
       "      <td>11553</td>\n",
       "      <td>Housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.0</th>\n",
       "      <td>11138</td>\n",
       "      <td>Science, Tech and Communications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>8779</td>\n",
       "      <td>Immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1731</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23.0</th>\n",
       "      <td>240</td>\n",
       "      <td>Arts and Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25.0</th>\n",
       "      <td>21</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24.0</th>\n",
       "      <td>10</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      majortopic01                             label\n",
       "20.0         68061             Government Operations\n",
       "16.0         55569                           Defense\n",
       "21.0         49245            Public Lands and Water\n",
       "3.0          39191                            Health\n",
       "19.0         33471     International Affairs and Aid\n",
       "12.0         32474                     Law and Crime\n",
       "15.0         31535       Finance & Domestic Commerce\n",
       "1.0          28497                           Economy\n",
       "10.0         25887                    Transportation\n",
       "18.0         20186                     Foreign Trade\n",
       "6.0          19916                         Education\n",
       "4.0          18991                       Agriculture\n",
       "7.0          18403                       Environment\n",
       "8.0          17743                            Energy\n",
       "13.0         17533                    Social Welfare\n",
       "5.0          17456              Labor and Employment\n",
       "2.0          15884                      Civil Rights\n",
       "14.0         11553                           Housing\n",
       "17.0         11138  Science, Tech and Communications\n",
       "9.0           8779                       Immigration\n",
       "0.0           1731                           unknown\n",
       "23.0           240            Arts and Entertainment\n",
       "25.0            21                           unknown\n",
       "24.0            10                           unknown"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_counts = pd.DataFrame(topic_tweets_[\"majortopic01\"].value_counts())\n",
    "topic_counts[\"label\"] = [topic_label_index.get(int(i), \"unknown\") for i in topic_counts.index]\n",
    "\n",
    "topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_topic = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted Size: (46620, 9)\n"
     ]
    }
   ],
   "source": [
    "topic_tweets = topic_tweets_[topic_tweets_.dataset.isin([\"all_tweets\", \"tweets\", \"ar-db\", \"emily-db\"])]\n",
    "topic_tweets = topic_tweets[topic_tweets.text.str.len() > 20]\n",
    "topic_tweets = topic_tweets[topic_tweets[\"majortopic01\"] > 0]\n",
    "\n",
    "topic_tweets.index = list(range(topic_tweets.shape[0]))\n",
    "print(\"Restricted Size:\", topic_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_tweet_texts = topic_tweets[\"text\"]\n",
    "topic_y_data = pd.Series([1 if int(x) == target_topic else 0 for x in topic_tweets[\"majortopic01\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But first, read in stopwrods\n",
    "enStop = stopwords.words('english')\n",
    "\n",
    "# Skip stop words, retweet signs, @ symbols, and URL headers\n",
    "stopList = enStop +\\\n",
    "    [\"http\", \"https\", \"rt\", \"@\", \":\", \"t.co\", \"co\", \"amp\", \"&amp;\", \"...\", \"\\n\", \"\\r\"]\n",
    "stopList.extend(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def tokenizer_wrapper(text):\n",
    "# #     return [t.lemma_ for t in nlp(text)]\n",
    "\n",
    "local_tokenizer = TweetTokenizer()\n",
    "def tokenizer_wrapper(text):\n",
    "    return local_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Additional Features\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "## Taken from Davidson et al.\n",
    "def other_features(tweet_text):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    \n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet_text)\n",
    "    \n",
    "    words = local_tokenizer.tokenize(tweet_text) #Get text only\n",
    "    \n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet_text)\n",
    "    num_terms = len(tweet_text.split())\n",
    "    num_words = len(words)\n",
    "    num_unique_terms = len(set([x.lower() for x in words]))\n",
    "    \n",
    "    caps_count = sum([1 if x.isupper() else 0 for x in tweet_text])\n",
    "    caps_ratio = caps_count / num_chars_total\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet_text) #Count #, @, and http://\n",
    "\n",
    "    features = [num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], \n",
    "                sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0]\n",
    "               ]\n",
    "\n",
    "    return features\n",
    "\n",
    "other_features_names = [\"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\n",
    "                        \"vader neu\", \"vader compound\", \\\n",
    "                        \"num_hashtags\", \"num_mentions\", \n",
    "                        \"num_urls\"\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clb617/anaconda3/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.2 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/clb617/anaconda3/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.2 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer_ = joblib.load(\"/home/clb617/Development/trecis/models/2013to2016_tfidf_vectorizer_20190109.pkl\")\n",
    "\n",
    "# tmp_vect = vectorizer\n",
    "tmp_vect = vectorizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {v:i for i, v in enumerate(tmp_vect.get_feature_names())}\n",
    "idf_vals = tmp_vect.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_state = 1337\n",
    "\n",
    "np.random.seed(r_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_tfidf = tmp_vect.transform(policy_tweet_texts).toarray()\n",
    "policy_other_ftr_data = np.array([other_features(tweet) for tweet in policy_tweet_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_X_data = np.concatenate([\n",
    "    policy_tfidf, \n",
    "    policy_other_ftr_data, \n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_nb_params = {\n",
    "    'alpha': 0.05134305647695325,\n",
    "    'binarize': 0.045909955637688404,\n",
    "    'fit_prior': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=0.05134305647695325, binarize=0.045909955637688404,\n",
       "      class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_fitted_model = BernoulliNB(**policy_nb_params)\n",
    "policy_fitted_model.fit(policy_X_data, policy_y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON to a tweet dict, returning None on fail for filtering\n",
    "def json_str_to_tweet(json_str):\n",
    "    tweet = None\n",
    "    try:\n",
    "        tweet = json.loads(json_str)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def get_tweet_text(tweet):\n",
    "    text = None\n",
    "    if ( \"full_text\" in tweet ):\n",
    "        text = tweet[\"full_text\"]\n",
    "    else:\n",
    "        text = tweet[\"text\"]\n",
    "    \n",
    "    return (tweet[\"id\"], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_broad = sc.broadcast(tmp_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.tokenizer import tokenizer_wrapper as tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tweets(local_texts):\n",
    "    \n",
    "    local_vect = vect_broad.value\n",
    "    vect_results = local_vect.transform(local_texts).toarray()\n",
    "    \n",
    "    other_ftr_data = np.array([other_features(tweet) for tweet in local_texts])\n",
    "    \n",
    "    local_X_data = np.concatenate([\n",
    "        vect_results, \n",
    "        other_ftr_data, \n",
    "    ], axis=1)\n",
    "    \n",
    "    return local_X_data\n",
    "\n",
    "def classify_tweets(tweet_iterator, clf):\n",
    "    \n",
    "    tweet_ids = []\n",
    "    local_texts = []\n",
    "    for tid, ttext in tweet_iterator:\n",
    "        tweet_ids.append(tid)\n",
    "        local_texts.append(ttext)\n",
    "    \n",
    "    if ( len(tweet_ids) > 0 ):\n",
    "        local_X_data = vectorize_tweets(local_texts)\n",
    "        labels = clf.predict(local_X_data).tolist()\n",
    "    \n",
    "        return zip(tweet_ids, labels)\n",
    "    \n",
    "    else:\n",
    "        return iter([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_sample_rdd_ = sc.textFile(\"usertimeline/random_us_timelines_sample\")\\\n",
    "    .map(json_str_to_tweet)\\\n",
    "    .filter(lambda t: t is not None)\\\n",
    "    .map(get_tweet_text)\\\n",
    "    .mapPartitions(lambda i: classify_tweets(i, policy_fitted_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_sample_ = poli_sample_rdd_.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_labels_df = pd.DataFrame(poli_sample_, columns=[\"tweet_id\", \"label\"])\n",
    "policy_labels_df.to_csv(\"policy_labels_twitter_political.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_labels_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_sample_rdd_ = sc.textFile(\"usertimeline/random_us_timelines_sample\")\\\n",
    "    .map(json_str_to_tweet)\\\n",
    "    .filter(lambda t: t is not None)\\\n",
    "    .map(get_tweet_text)\\\n",
    "    .mapPartitions(lambda i: classify_tweets(i, policy_fitted_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_sample_ = poli_sample_rdd_.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_labels_df = pd.DataFrame(poli_sample_, columns=[\"tweet_id\", \"label\"])\n",
    "policy_labels_df.to_csv(\"policy_labels_twitter_random.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6142464\n",
       "1    4386260\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_labels_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clb617/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Read CSV of tweets\n",
    "ira_df = pd.read_csv(\"/scratch/olympus/projects/elections_integrity/ira/tweets/ira_tweets_csv_hashed_orig.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(877919995476496385,\n",
       "  'RT @ruopentwit: ⚡️У НАС НОВОЕ ВИДЕО! Американец: \"Если бы не 27 миллионов русских, я бы сейчас говорил по-немецки\" https://t.co/mAcCirn4o1…'),\n",
       " (492388766930444288, 'Серебром отколоколило http://t.co/Jaa4v4IFpM'),\n",
       " (719455077589721089, '@kpru С-300 в Иране https://t.co/elnu3qLUW7'),\n",
       " (536179342423105537,\n",
       "  'Предлагаю судить их за поддержку нацизма, т.к. они отказались его осуждать!! #STOPNazi'),\n",
       " (841410788409630720,\n",
       "  'Предостережение американского дипломата https://t.co/fKPBVgIoVc')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t[1], t[2]) for t in ira_df[[\"tweetid\", \"tweet_text\"]].head().itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ira_sample_rdd_ = sc.parallelize(\n",
    "    [(t[1], t[2]) for t in ira_df[[\"tweetid\", \"tweet_text\"]].dropna().itertuples()],\n",
    "    512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ira_poli_labels = ira_sample_rdd_\\\n",
    "    .mapPartitions(lambda i: classify_tweets(i, policy_fitted_model))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ira_policy_labels_df = pd.DataFrame(ira_poli_labels, columns=[\"tweet_id\", \"label\"])\n",
    "ira_policy_labels_df.to_csv(\"policy_labels_twitter_ira.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    7108631\n",
       "0    1932675\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ira_policy_labels_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply This To Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON to a dict, returning None on fail for filtering\n",
    "def json_str_to_reddit_sub(json_str):\n",
    "    submission = None\n",
    "    try:\n",
    "        submission = json.loads(json_str)\n",
    "        \n",
    "        # Throw away comments. All submissions should have an is_self field\n",
    "        if ( \"is_self\" not in submission ):\n",
    "            submission = None\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_text(sub):\n",
    "    sub_id = sub[\"id\"]\n",
    "    text = sub[\"title\"]\n",
    "    \n",
    "    if text == \"[deleted]\":\n",
    "        text = \"\"\n",
    "        \n",
    "    if ( \"selftext\" in sub and len(sub[\"selftext\"]) > 0 and sub[\"selftext\"] != \"[deleted]\" ):\n",
    "        text = text + \". \" + sub[\"selftext\"]\n",
    "\n",
    "    return (sub_id, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: reddit_troll_submissions\n",
      "1    9121\n",
      "0    4767\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for k, p in [\n",
    "    (\"policy_labels_reddit_random.csv\", \"usertimeline/reddit_random_sample\"),\n",
    "    (\"policy_labels_reddit_poli.csv\", \"usertimeline/reddit_political_sample\"),\n",
    "    (\"policy_labels_reddit_ira.csv\", \"reddit_troll_submissions\"),\n",
    "]:\n",
    "    print(\"Processing:\", p)\n",
    "\n",
    "    local_sample_rdd = sc.textFile(p)\\\n",
    "        .map(json_str_to_reddit_sub)\\\n",
    "        .filter(lambda t: t is not None)\\\n",
    "        .map(get_reddit_text)\\\n",
    "        .filter(lambda t: len(t[1]) > 0)\\\n",
    "        .mapPartitions(lambda i: classify_tweets(i, policy_fitted_model))\n",
    "\n",
    "    local_sample_ = local_sample_rdd.collect()\n",
    "    \n",
    "    policy_labels_df = pd.DataFrame(local_sample_, columns=[\"sub_id\", \"label\"])\n",
    "    policy_labels_df.to_csv(k, index=False)\n",
    "    \n",
    "    print(policy_labels_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_labels_df[\"url\"] = policy_labels_df[\"sub_id\"].apply(lambda x: \"http://redd.it/\" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_id</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>k2446</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/k2446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k3s6p</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/k3s6p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k7dmo</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/k7dmo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kc026</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/kc026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kcz44</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/kcz44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ki807</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/ki807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sfp8h</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/sfp8h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>v22nt</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/v22nt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wr1in</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/wr1in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10bzl6</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/10bzl6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13kwwe</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/13kwwe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14g3yd</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/14g3yd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1gbuur</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/1gbuur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1ielr4</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/1ielr4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1khcv6</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/1khcv6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1qe98q</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/1qe98q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1sb0mk</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/1sb0mk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1xaced</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/1xaced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1ys4kx</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/1ys4kx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1zpj0a</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/1zpj0a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20b8b5</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/20b8b5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22688y</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/22688y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22ek08</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/22ek08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22ms90</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/22ms90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>22rp22</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/22rp22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>22uuuz</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/22uuuz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23pjqp</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/23pjqp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>248pzh</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/248pzh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>24jdx0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/24jdx0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24lyhr</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/24lyhr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13858</th>\n",
       "      <td>7amcpl</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/7amcpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13859</th>\n",
       "      <td>7ano10</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/7ano10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13860</th>\n",
       "      <td>7b66uv</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/7b66uv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13861</th>\n",
       "      <td>7bnt19</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7bnt19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13862</th>\n",
       "      <td>7bnvun</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7bnvun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13863</th>\n",
       "      <td>7bnyjp</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7bnyjp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13864</th>\n",
       "      <td>7bnzm7</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7bnzm7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13865</th>\n",
       "      <td>7bo2xb</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7bo2xb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13866</th>\n",
       "      <td>7bo45w</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/7bo45w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13867</th>\n",
       "      <td>7bo6xf</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7bo6xf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13868</th>\n",
       "      <td>7bqwgt</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7bqwgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13869</th>\n",
       "      <td>7bruq2</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/7bruq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13870</th>\n",
       "      <td>7bwj2w</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7bwj2w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13871</th>\n",
       "      <td>7c5z32</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7c5z32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13872</th>\n",
       "      <td>7ca9ku</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/7ca9ku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13873</th>\n",
       "      <td>7cfn6g</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7cfn6g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13874</th>\n",
       "      <td>7cfnuv</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7cfnuv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13875</th>\n",
       "      <td>7cfoic</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7cfoic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13876</th>\n",
       "      <td>7cfp8f</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7cfp8f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13877</th>\n",
       "      <td>7cgadd</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/7cgadd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13878</th>\n",
       "      <td>7cgcct</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7cgcct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13879</th>\n",
       "      <td>7cggki</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/7cggki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13880</th>\n",
       "      <td>7ci2xf</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/7ci2xf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13881</th>\n",
       "      <td>7e6dcd</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7e6dcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13882</th>\n",
       "      <td>7eahnc</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7eahnc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13883</th>\n",
       "      <td>7extlm</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7extlm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13884</th>\n",
       "      <td>7fpt45</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7fpt45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13885</th>\n",
       "      <td>7g3gsp</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7g3gsp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13886</th>\n",
       "      <td>7gac9d</td>\n",
       "      <td>0</td>\n",
       "      <td>http://redd.it/7gac9d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13887</th>\n",
       "      <td>7gkt0l</td>\n",
       "      <td>1</td>\n",
       "      <td>http://redd.it/7gkt0l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13888 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sub_id  label                    url\n",
       "0       k2446      0   http://redd.it/k2446\n",
       "1       k3s6p      0   http://redd.it/k3s6p\n",
       "2       k7dmo      0   http://redd.it/k7dmo\n",
       "3       kc026      1   http://redd.it/kc026\n",
       "4       kcz44      1   http://redd.it/kcz44\n",
       "5       ki807      1   http://redd.it/ki807\n",
       "6       sfp8h      1   http://redd.it/sfp8h\n",
       "7       v22nt      1   http://redd.it/v22nt\n",
       "8       wr1in      1   http://redd.it/wr1in\n",
       "9      10bzl6      0  http://redd.it/10bzl6\n",
       "10     13kwwe      1  http://redd.it/13kwwe\n",
       "11     14g3yd      1  http://redd.it/14g3yd\n",
       "12     1gbuur      0  http://redd.it/1gbuur\n",
       "13     1ielr4      1  http://redd.it/1ielr4\n",
       "14     1khcv6      1  http://redd.it/1khcv6\n",
       "15     1qe98q      1  http://redd.it/1qe98q\n",
       "16     1sb0mk      1  http://redd.it/1sb0mk\n",
       "17     1xaced      1  http://redd.it/1xaced\n",
       "18     1ys4kx      0  http://redd.it/1ys4kx\n",
       "19     1zpj0a      1  http://redd.it/1zpj0a\n",
       "20     20b8b5      1  http://redd.it/20b8b5\n",
       "21     22688y      0  http://redd.it/22688y\n",
       "22     22ek08      1  http://redd.it/22ek08\n",
       "23     22ms90      1  http://redd.it/22ms90\n",
       "24     22rp22      1  http://redd.it/22rp22\n",
       "25     22uuuz      1  http://redd.it/22uuuz\n",
       "26     23pjqp      1  http://redd.it/23pjqp\n",
       "27     248pzh      1  http://redd.it/248pzh\n",
       "28     24jdx0      0  http://redd.it/24jdx0\n",
       "29     24lyhr      1  http://redd.it/24lyhr\n",
       "...       ...    ...                    ...\n",
       "13858  7amcpl      0  http://redd.it/7amcpl\n",
       "13859  7ano10      0  http://redd.it/7ano10\n",
       "13860  7b66uv      0  http://redd.it/7b66uv\n",
       "13861  7bnt19      1  http://redd.it/7bnt19\n",
       "13862  7bnvun      1  http://redd.it/7bnvun\n",
       "13863  7bnyjp      1  http://redd.it/7bnyjp\n",
       "13864  7bnzm7      1  http://redd.it/7bnzm7\n",
       "13865  7bo2xb      1  http://redd.it/7bo2xb\n",
       "13866  7bo45w      0  http://redd.it/7bo45w\n",
       "13867  7bo6xf      1  http://redd.it/7bo6xf\n",
       "13868  7bqwgt      1  http://redd.it/7bqwgt\n",
       "13869  7bruq2      0  http://redd.it/7bruq2\n",
       "13870  7bwj2w      1  http://redd.it/7bwj2w\n",
       "13871  7c5z32      1  http://redd.it/7c5z32\n",
       "13872  7ca9ku      0  http://redd.it/7ca9ku\n",
       "13873  7cfn6g      1  http://redd.it/7cfn6g\n",
       "13874  7cfnuv      1  http://redd.it/7cfnuv\n",
       "13875  7cfoic      1  http://redd.it/7cfoic\n",
       "13876  7cfp8f      1  http://redd.it/7cfp8f\n",
       "13877  7cgadd      0  http://redd.it/7cgadd\n",
       "13878  7cgcct      1  http://redd.it/7cgcct\n",
       "13879  7cggki      0  http://redd.it/7cggki\n",
       "13880  7ci2xf      0  http://redd.it/7ci2xf\n",
       "13881  7e6dcd      1  http://redd.it/7e6dcd\n",
       "13882  7eahnc      1  http://redd.it/7eahnc\n",
       "13883  7extlm      1  http://redd.it/7extlm\n",
       "13884  7fpt45      1  http://redd.it/7fpt45\n",
       "13885  7g3gsp      1  http://redd.it/7g3gsp\n",
       "13886  7gac9d      0  http://redd.it/7gac9d\n",
       "13887  7gkt0l      1  http://redd.it/7gkt0l\n",
       "\n",
       "[13888 rows x 3 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweetid                       int64\n",
       "userid                       object\n",
       "user_display_name            object\n",
       "user_screen_name             object\n",
       "user_reported_location       object\n",
       "user_profile_description     object\n",
       "user_profile_url             object\n",
       "follower_count                int64\n",
       "following_count               int64\n",
       "account_creation_date        object\n",
       "account_language             object\n",
       "tweet_language               object\n",
       "tweet_text                   object\n",
       "tweet_time                   object\n",
       "tweet_client_name            object\n",
       "in_reply_to_tweetid         float64\n",
       "in_reply_to_userid           object\n",
       "quoted_tweet_tweetid        float64\n",
       "is_retweet                     bool\n",
       "retweet_userid               object\n",
       "retweet_tweetid             float64\n",
       "latitude                    float64\n",
       "longitude                   float64\n",
       "quote_count                 float64\n",
       "reply_count                 float64\n",
       "like_count                  float64\n",
       "retweet_count               float64\n",
       "hashtags                     object\n",
       "urls                         object\n",
       "user_mentions                object\n",
       "poll_choices                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ira_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
