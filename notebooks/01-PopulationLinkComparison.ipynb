{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Twitter and Reddit Populations\n",
    "\n",
    "This script evaluates how similar subsets of Twitter and Reddit populations are within platform and across platform.\n",
    "The populations we test are:\n",
    "- Random US Twitter users active in 2015-2018\n",
    "    - Must have tweeted >= 100 times in this time frame\n",
    "- Politically engaged US Twitter users active in 2015-2018\n",
    "    - Must have tweeted >= 100 times in this time frame\n",
    "    - Must follow at least 5 politicians\n",
    "- IRA Twitter accounts identified by Twitter\n",
    "- Random Reddit accounts  active in 2015-2018\n",
    "    - Must have posted/commented >= 100 times in this time frame\n",
    "- Politically engaged Redditors  active in 2015-2018\n",
    "    - Must have posted/commented >= 100 times in subreddits /r/politics identifies as US-political in this time frame\n",
    "- IRA Reddit accounts identified by Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import string\n",
    "import datetime\n",
    "import matplotlib\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.youtube import strip_video_id_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date):\n",
    "    return datetime.datetime.strptime(date, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "\n",
    "def convert_time(timestamp):\n",
    "    created_time = timestamp\n",
    "    if ( isinstance(created_time, str) ):\n",
    "        created_time = int(created_time)\n",
    "        \n",
    "    d = datetime.datetime.utcfromtimestamp(created_time)\n",
    "    return d\n",
    "\n",
    "def get_top_tlds(links_df, user_id_field, top_n=100):\n",
    "    tld_user_counts = {}\n",
    "    for tld, group in links_df.groupby(\"tld\"):\n",
    "        local_users = set(group[user_id_field])\n",
    "        tld_user_counts[tld] = len(local_users)\n",
    "\n",
    "    return sorted(tld_user_counts, key=tld_user_counts.get, reverse=True)[:top_n]\n",
    "\n",
    "def links_df_to_shares(links_df, domains, user_id_field):\n",
    "    user_shares = []\n",
    "\n",
    "    for user,user_posts in links_df.groupby(user_id_field):\n",
    "        local_tld_counts = dict(user_posts[\"tld\"].value_counts().items())\n",
    "        local_vector = [local_tld_counts.get(x, 0) for x in domains]\n",
    "        user_shares.append([user] + local_vector)\n",
    "\n",
    "    return pd.DataFrame(user_shares, columns=[user_id_field] + domains)\n",
    "\n",
    "def links_to_norm_matrix(links_df, domains, user_id_field):\n",
    "    if ( type(domains) == set ):\n",
    "        domains = list(domains)\n",
    "        \n",
    "    shares_df = links_df_to_shares(links_df, domains, user_id_field)\n",
    "    return sklearn.preprocessing.normalize(shares_df[domains], norm=\"l2\", axis=1)\n",
    "\n",
    "def get_top_channels(yt_df, user_id_field, top_n=100):\n",
    "    channel_user_counts = {}\n",
    "    for channel, group in yt_df.groupby(\"channel_id\"):\n",
    "        local_users = set(group[user_id_field])\n",
    "        channel_user_counts[channel] = len(local_users)\n",
    "\n",
    "    return sorted(channel_user_counts, key=channel_user_counts.get, reverse=True)[:top_n]\n",
    "\n",
    "def channels_df_to_shares(links_df, channels, user_id_field):\n",
    "    user_shares = []\n",
    "\n",
    "    for user,user_posts in links_df.groupby(user_id_field):\n",
    "        local_chan_counts = dict(user_posts[\"channel_id\"].value_counts().items())\n",
    "        local_vector = [local_chan_counts.get(x, 0) for x in channels]\n",
    "        user_shares.append([user] + local_vector)\n",
    "\n",
    "    return pd.DataFrame(user_shares, columns=[user_id_field] + channels)\n",
    "\n",
    "def channels_to_norm_matrix(links_df, channels, user_id_field):\n",
    "    if ( type(channels) == set ):\n",
    "        channels = list(channels)\n",
    "        \n",
    "    shares_df = channels_df_to_shares(links_df, channels, user_id_field)\n",
    "    return sklearn.preprocessing.normalize(shares_df[channels], norm=\"l2\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "political_domains_df = pd.read_csv(\"DomainIdeology.csv\")\n",
    "political_domains = set(political_domains_df[\"domain\"].apply(str.lower))\n",
    "print(\"Political Domains:\", len(political_domains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_domains = 150\n",
    "top_n_channels = 250\n",
    "bootstrap_count = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_domain_map = {\n",
    "    \"abcn.ws\": \"abcnews.go.com\",\n",
    "    \"amzn.to\": \"amazon.com\",\n",
    "    \"apne.ws\": \"apnews.com\",\n",
    "    \"apple.co\": \"apple.com\",\n",
    "    \"bbc.in\": \"bbc.com\",\n",
    "    \"ble.ac\": \"bleacherreport.com\",\n",
    "    \"bloom.bg\": \"bloomberg.com\",\n",
    "    \"bzfd.it\": \"buzzfeed.com\",\n",
    "    \"cbsloc.al\": \"cbslocal.com\",\n",
    "    \"cnb.cx\": \"cnbc.com\",\n",
    "    \"cnn.it\": \"cnn.com\",\n",
    "    \"cnn.it\": \"cnn.com\",\n",
    "    \"dailym.ai\" : \"dailymail.co.uk\",\n",
    "    \"econ.st\": \"economist.com\",\n",
    "    \"es.pn\": \"espn.com\",\n",
    "    \"fxn.ws\": \"foxnews.com\",\n",
    "    \"hill.cm\": \"thehill.com\",\n",
    "    \"huff.to\" : \"huffingtonpost.com\",\n",
    "    \"lat.ms\": \"latimes.com\",\n",
    "    \"lnkd.in\": \"linkedin.com\",\n",
    "    \"n.pr\": \"npr.org\",\n",
    "    \"nbcnews.to\": \"nbcnews.com\",\n",
    "    \"nydn.us\": \"nydailynews.com\",\n",
    "    \"nyp.st\": \"nypost.com\",\n",
    "    \"nyti.ms\": \"nytimes.com\",\n",
    "    \"on.rt.com\": \"rt.com\",\n",
    "    \"on.wsj.com\": \"wsj.com\",\n",
    "    \"politi.co\": \"politico.com\",\n",
    "    \"redd.it\": \"reddit.com\",\n",
    "    \"reut.rs\": \"reuters.com\",\n",
    "    \"thebea.st\": \"thedailybeast.com\",\n",
    "    \"ti.me\": \"time.com\",\n",
    "    \"tmblr.co\": \"tumblr.com\",\n",
    "    \"usat.ly\": \"usatoday.com\",\n",
    "    \"wapo.st\": \"washingtonpost.com\",\n",
    "    \"wp.me\": \"wordpress.com\",\n",
    "    \"wpo.st\": \"washingtonpost.com\",\n",
    "    \"yhoo.it\": \"yahoo.com\",\n",
    "    \"youtu.be\": \"youtube.com\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_links_df_rand = pd.read_csv(\"twitter_random_us_sample_links.csv\",\n",
    "    converters={\"created_at\": convert_date})\n",
    "twitter_links_df_poli = pd.read_csv(\"twitter_political_us_sample_links.csv\",\n",
    "    converters={\"created_at\": convert_date})\n",
    "twitter_links_df_ira = pd.read_csv(\n",
    "    \"twitter_ira_links.csv\",\n",
    "    converters={\"created_at\": lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_links_df_rand[\"tld\"] = twitter_links_df_rand[\"tld\"].apply(lambda x: short_domain_map.get(x, x))\n",
    "twitter_links_df_poli[\"tld\"] = twitter_links_df_poli[\"tld\"].apply(lambda x: short_domain_map.get(x, x))\n",
    "twitter_links_df_ira[\"tld\"] = twitter_links_df_ira[\"tld\"].apply(lambda x: short_domain_map.get(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "twitter_top_tlds_rand = get_top_tlds(twitter_links_df_rand, user_id_field=\"user_id\", top_n=top_n_domains)\n",
    "twitter_top_tlds_poli = get_top_tlds(twitter_links_df_poli, user_id_field=\"user_id\", top_n=top_n_domains)\n",
    "twitter_top_tlds_ira = get_top_tlds(twitter_links_df_ira, user_id_field=\"user_id\", top_n=top_n_domains)\n",
    "\n",
    "top_tld_map_twitter = {\n",
    "    \"random\": set(twitter_top_tlds_rand),\n",
    "    \"political\": set(twitter_top_tlds_poli),\n",
    "    \"ira\": set(twitter_top_tlds_ira),\n",
    "}\n",
    "\n",
    "for population, tlds in top_tld_map_twitter.items():\n",
    "    print(population, \":\", len(tlds))\n",
    "\n",
    "# twitter_merged_tlds = set(twitter_top_tlds_rand)\\\n",
    "#     .intersection(twitter_top_tlds_poli)\n",
    "# #     .intersection(twitter_top_tlds_ira)\n",
    "# print(\"Merged Top TLDs:\", len(twitter_merged_tlds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_links_df_rand = pd.read_csv(\"reddit_random_links.csv\",\n",
    "    converters={\"created_at\": convert_time})\n",
    "reddit_links_df_poli = pd.read_csv(\"reddit_political_links.csv\",\n",
    "    converters={\"created_at\": convert_time})\n",
    "reddit_links_df_ira = pd.read_csv(\"reddit_troll_links.csv\",\n",
    "    converters={\"created_at\": convert_time}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_links_df_rand[\"tld\"] = reddit_links_df_rand[\"tld\"].apply(lambda x: short_domain_map.get(x, x))\n",
    "reddit_links_df_poli[\"tld\"] = reddit_links_df_poli[\"tld\"].apply(lambda x: short_domain_map.get(x, x))\n",
    "reddit_links_df_ira[\"tld\"] = reddit_links_df_ira[\"tld\"].apply(lambda x: short_domain_map.get(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reddit_top_tlds_rand = get_top_tlds(reddit_links_df_rand, user_id_field=\"user_name\", top_n=top_n_domains)\n",
    "reddit_top_tlds_poli = get_top_tlds(reddit_links_df_poli, user_id_field=\"user_name\", top_n=top_n_domains)\n",
    "reddit_top_tlds_ira = get_top_tlds(reddit_links_df_ira, user_id_field=\"user_name\", top_n=top_n_domains)\n",
    "\n",
    "top_tld_map_reddit = {\n",
    "    \"random\": set(reddit_top_tlds_rand),\n",
    "    \"political\": set(reddit_top_tlds_poli),\n",
    "    \"ira\": set(reddit_top_tlds_ira),\n",
    "}\n",
    "\n",
    "for population, tlds in top_tld_map_reddit.items():\n",
    "    print(population, \":\", len(tlds))\n",
    "\n",
    "# reddit_merged_tlds = set(reddit_top_tlds_rand)\\\n",
    "#     .intersection(reddit_top_tlds_poli)\n",
    "# #     .intersection(reddit_top_tlds_ira)\n",
    "# print(\"Merged Top TLDs:\", len(reddit_merged_tlds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_platform_tlds = twitter_merged_tlds.intersection(reddit_merged_tlds)\n",
    "\n",
    "# # Pop off these ultra-common TLDs. \n",
    "# #. The motivation here is that twitter.com is injected \n",
    "# #.  when a Twitter user retweets, and sharing within \n",
    "# #.  Reddit through crossposting increases its prevalence.\n",
    "# #.  These phenomena will make activity across populations\n",
    "# #.  appear artificially more similar than they ought to be.\n",
    "# # cross_platform_tlds.remove(\"twitter.com\")\n",
    "# # cross_platform_tlds.remove(\"reddit.com\")\n",
    "\n",
    "# print(\"Cross-Platform Top TLDs:\", len(cross_platform_tlds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Cross-Platform TLDs:\")\n",
    "# for x in sorted(cross_platform_tlds):\n",
    "#     print(\"\\t\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"TLDs in Reddit but not Twitter:\")\n",
    "# for tld in sorted(reddit_merged_tlds.difference(twitter_merged_tlds)):\n",
    "#     print(\"\\t\", tld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"TLDs in Twitter but not Reddit:\")\n",
    "# for tld in sorted(twitter_merged_tlds.difference(reddit_merged_tlds)):\n",
    "#     print(\"\\t\", tld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following to test whether focusing only on political domains \n",
    "#. changes outcomes\n",
    "# top_tld_map_twitter = {k: political_domains for k in top_tld_map_twitter}\n",
    "# top_tld_map_reddit = {k: political_domains for k in top_tld_map_reddit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_overlap_rand_poli = top_tld_map_twitter[\"random\"].intersection(top_tld_map_twitter[\"political\"])\n",
    "twitter_overlap_rand_ira = top_tld_map_twitter[\"random\"].intersection(top_tld_map_twitter[\"ira\"])\n",
    "twitter_overlap_poli_ira = top_tld_map_twitter[\"political\"].intersection(top_tld_map_twitter[\"ira\"])\n",
    "\n",
    "twitter_user_links_mat_rand = links_to_norm_matrix(twitter_links_df_rand, top_tld_map_twitter[\"random\"], \"user_id\")\n",
    "twitter_user_links_mat_poli = links_to_norm_matrix(twitter_links_df_poli, top_tld_map_twitter[\"political\"], \"user_id\")\n",
    "twitter_user_links_mat_ira = links_to_norm_matrix(twitter_links_df_ira, top_tld_map_twitter[\"ira\"], \"user_id\")\n",
    "\n",
    "# Calculate pairwise similarity among users across populations\n",
    "t2t_rand_rand_sim = sklearn.metrics.pairwise.cosine_similarity(twitter_user_links_mat_rand, twitter_user_links_mat_rand)\n",
    "t2t_poli_poli_sim = sklearn.metrics.pairwise.cosine_similarity(twitter_user_links_mat_poli, twitter_user_links_mat_poli)\n",
    "t2t_ira_ira_sim = sklearn.metrics.pairwise.cosine_similarity(twitter_user_links_mat_ira, twitter_user_links_mat_ira)\n",
    "\n",
    "t2t_ira_rand_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    links_to_norm_matrix(twitter_links_df_ira, twitter_overlap_rand_ira, \"user_id\"), \n",
    "    links_to_norm_matrix(twitter_links_df_rand, twitter_overlap_rand_ira, \"user_id\"))\n",
    "t2t_ira_poli_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    links_to_norm_matrix(twitter_links_df_ira, twitter_overlap_poli_ira, \"user_id\"), \n",
    "    links_to_norm_matrix(twitter_links_df_poli, twitter_overlap_poli_ira, \"user_id\"))\n",
    "t2t_rand_poli_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    links_to_norm_matrix(twitter_links_df_rand, twitter_overlap_rand_poli, \"user_id\"), \n",
    "    links_to_norm_matrix(twitter_links_df_poli, twitter_overlap_rand_poli, \"user_id\"))\n",
    "\n",
    "# Collapse similarities down to get the mean similarity for each user on the left to all users on the right\n",
    "#. note the minus 1 and reduction of shape by 1 for the within-platform groups, which I do to remove the \n",
    "#. self-similarity effect\n",
    "t2t_rand_rand_sim_avg = (np.sum(t2t_rand_rand_sim, axis=1) - 1) / (twitter_user_links_mat_rand.shape[0] - 1)\n",
    "t2t_poli_poli_sim_avg = (np.sum(t2t_poli_poli_sim, axis=1) - 1) / (twitter_user_links_mat_poli.shape[0] - 1)\n",
    "t2t_ira_ira_sim_avg = (np.sum(t2t_ira_ira_sim, axis=1) - 1) / (twitter_user_links_mat_ira.shape[0] - 1)\n",
    "t2t_ira_rand_sim_avg = np.mean(t2t_ira_rand_sim, axis=1)\n",
    "t2t_ira_poli_sim_avg = np.mean(t2t_ira_poli_sim, axis=1)\n",
    "t2t_ira_ira_sim_avg = np.mean(t2t_ira_ira_sim, axis=1)\n",
    "t2t_rand_poli_sim_avg = np.sum(t2t_rand_poli_sim, axis=1)\n",
    "\n",
    "# # Plot what these similarities look like\n",
    "# plt.hist(t2t_ira_rand_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-rand\")\n",
    "# plt.hist(t2t_ira_poli_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-poli\")\n",
    "# plt.hist(t2t_ira_ira_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Show similarity distribution within populations in this platform\n",
    "t2t_rand_rand_sim_avg_bootstrap = [sklearn.utils.resample(t2t_rand_rand_sim_avg, replace=True, n_samples=t2t_rand_rand_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "t2t_poli_poli_sim_avg_bootstrap = [sklearn.utils.resample(t2t_poli_poli_sim_avg, replace=True, n_samples=t2t_poli_poli_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "t2t_ira_ira_sim_avg_bootstrap = [sklearn.utils.resample(t2t_ira_ira_sim_avg, replace=True, n_samples=t2t_ira_ira_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "\n",
    "plt.hist(t2t_rand_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"rand-rand\")\n",
    "plt.hist(t2t_poli_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"poli-poli\")\n",
    "plt.hist(t2t_ira_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "plt.title(\"Within-Population Similarity\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Show similarity distribution across populations in this platform\n",
    "t2t_ira_rand_sim_avg_bootstrap = [sklearn.utils.resample(t2t_ira_rand_sim_avg, replace=True, n_samples=t2t_ira_rand_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "t2t_ira_poli_sim_avg_bootstrap = [sklearn.utils.resample(t2t_ira_poli_sim_avg, replace=True, n_samples=t2t_ira_poli_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "t2t_ira_ira_sim_avg_bootstrap = [sklearn.utils.resample(t2t_ira_ira_sim_avg, replace=True, n_samples=t2t_ira_ira_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "\n",
    "plt.hist(t2t_ira_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-rand\")\n",
    "plt.hist(t2t_ira_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-poli\")\n",
    "plt.hist(t2t_ira_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "plt.title(\"Across-Population Similarity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether the within-group similarities across the three populations are equal\n",
    "f_stat, p_val = scipy.stats.f_oneway(t2t_rand_rand_sim_avg_bootstrap, t2t_poli_poli_sim_avg_bootstrap, t2t_ira_ira_sim_avg_bootstrap)\n",
    "print(\"p-value for ANOVA:\", p_val, f_stat)\n",
    "\n",
    "t_stat, p_val = scipy.stats.ttest_ind(t2t_poli_poli_sim_avg_bootstrap, t2t_ira_ira_sim_avg_bootstrap, axis=0, equal_var=False)\n",
    "print(\"p-value for Welch's t-Test between within-Poli and within-IRA:\", p_val, t_stat)\n",
    "\n",
    "t_stat, p_val = scipy.stats.ttest_ind(t2t_ira_rand_sim_avg_bootstrap, t2t_ira_poli_sim_avg_bootstrap, axis=0, equal_var=True)\n",
    "print(\"p-value for Welch's t-Test between IRA-Rand and IRA-Poli:\", p_val, t_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(twitter_user_links_mat_rand.T)\n",
    "plt.show()\n",
    "plt.imshow(twitter_user_links_mat_poli.T)\n",
    "plt.show()\n",
    "plt.imshow(twitter_user_links_mat_ira.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, mat in [\n",
    "    (\"random\", twitter_user_links_mat_rand), \n",
    "    (\"political\", twitter_user_links_mat_poli), \n",
    "    (\"ira\", twitter_user_links_mat_ira)]:\n",
    "    print(label)\n",
    "    domains = top_tld_map_twitter[label]\n",
    "    for tld, prop in sorted(zip(domains, np.mean(mat, axis=0)), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(\"\\t\", tld, prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_overlap_rand_poli = top_tld_map_reddit[\"random\"].intersection(top_tld_map_reddit[\"political\"])\n",
    "reddit_overlap_rand_ira = top_tld_map_reddit[\"random\"].intersection(top_tld_map_reddit[\"ira\"])\n",
    "reddit_overlap_poli_ira = top_tld_map_reddit[\"political\"].intersection(top_tld_map_reddit[\"ira\"])\n",
    "\n",
    "reddit_user_links_mat_rand = links_to_norm_matrix(reddit_links_df_rand, top_tld_map_reddit[\"random\"], \"user_name\")\n",
    "reddit_user_links_mat_poli = links_to_norm_matrix(reddit_links_df_poli, top_tld_map_reddit[\"political\"], \"user_name\")\n",
    "reddit_user_links_mat_ira = links_to_norm_matrix(reddit_links_df_ira, top_tld_map_reddit[\"ira\"], \"user_name\")\n",
    "\n",
    "# Calculate pairwise similarity among users across populations\n",
    "r2r_rand_rand_sim = sklearn.metrics.pairwise.cosine_similarity(reddit_user_links_mat_rand, reddit_user_links_mat_rand)\n",
    "r2r_poli_poli_sim = sklearn.metrics.pairwise.cosine_similarity(reddit_user_links_mat_poli, reddit_user_links_mat_poli)\n",
    "r2r_ira_ira_sim = sklearn.metrics.pairwise.cosine_similarity(reddit_user_links_mat_ira, reddit_user_links_mat_ira)\n",
    "\n",
    "r2r_ira_rand_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    links_to_norm_matrix(reddit_links_df_ira, reddit_overlap_rand_ira, \"user_name\"), \n",
    "    links_to_norm_matrix(reddit_links_df_rand, reddit_overlap_rand_ira, \"user_name\"))\n",
    "r2r_ira_poli_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    links_to_norm_matrix(reddit_links_df_ira, reddit_overlap_poli_ira, \"user_name\"), \n",
    "    links_to_norm_matrix(reddit_links_df_poli, reddit_overlap_poli_ira, \"user_name\"))\n",
    "r2r_rand_poli_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    links_to_norm_matrix(reddit_links_df_rand, reddit_overlap_rand_poli, \"user_name\"), \n",
    "    links_to_norm_matrix(reddit_links_df_poli, reddit_overlap_rand_poli, \"user_name\"))\n",
    "\n",
    "# Collapse similarities down to get the mean similarity for each user on the left to all users on the right\n",
    "#. note the minus 1 and reduction of shape by 1 for the within-platform groups, which I do to remove the \n",
    "#. self-similarity effect\n",
    "r2r_rand_rand_sim_avg = (np.sum(r2r_rand_rand_sim, axis=1) - 1) / (reddit_user_links_mat_rand.shape[0] - 1)\n",
    "r2r_poli_poli_sim_avg = (np.sum(r2r_poli_poli_sim, axis=1) - 1) / (reddit_user_links_mat_poli.shape[0] - 1)\n",
    "r2r_ira_ira_sim_avg = (np.sum(r2r_ira_ira_sim, axis=1) - 1) / (reddit_user_links_mat_ira.shape[0] - 1)\n",
    "r2r_ira_rand_sim_avg = np.mean(r2r_ira_rand_sim, axis=1)\n",
    "r2r_ira_poli_sim_avg = np.mean(r2r_ira_poli_sim, axis=1)\n",
    "r2r_ira_ira_sim_avg = np.mean(r2r_ira_ira_sim, axis=1)\n",
    "r2r_rand_poli_sim_avg = np.sum(r2r_rand_poli_sim, axis=1)\n",
    "\n",
    "# # Plot what these similarities look like\n",
    "# plt.hist(r2r_ira_rand_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-rand\")\n",
    "# plt.hist(r2r_ira_poli_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-poli\")\n",
    "# plt.hist(r2r_ira_ira_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Show similarity distribution within populations in this platform\n",
    "r2r_rand_rand_sim_avg_bootstrap = [sklearn.utils.resample(r2r_rand_rand_sim_avg, replace=True, n_samples=r2r_rand_rand_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2r_poli_poli_sim_avg_bootstrap = [sklearn.utils.resample(r2r_poli_poli_sim_avg, replace=True, n_samples=r2r_poli_poli_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2r_ira_ira_sim_avg_bootstrap = [sklearn.utils.resample(r2r_ira_ira_sim_avg, replace=True, n_samples=r2r_ira_ira_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "\n",
    "plt.hist(r2r_rand_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"rand-rand\")\n",
    "plt.hist(r2r_poli_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"poli-poli\")\n",
    "plt.hist(r2r_ira_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "plt.title(\"Within-Population Similarity\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Show similarity distribution across populations in this platform\n",
    "r2r_ira_rand_sim_avg_bootstrap = [sklearn.utils.resample(r2r_ira_rand_sim_avg, replace=True, n_samples=r2r_ira_rand_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2r_ira_poli_sim_avg_bootstrap = [sklearn.utils.resample(r2r_ira_poli_sim_avg, replace=True, n_samples=r2r_ira_poli_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2r_ira_ira_sim_avg_bootstrap = [sklearn.utils.resample(r2r_ira_ira_sim_avg, replace=True, n_samples=r2r_ira_ira_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "\n",
    "plt.hist(r2r_ira_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-rand\")\n",
    "plt.hist(r2r_ira_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-poli\")\n",
    "plt.hist(r2r_ira_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "plt.title(\"Across-Population Similarity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether the within-group similarities across the three populations are equal\n",
    "f_stat, p_val = scipy.stats.f_oneway(r2r_rand_rand_sim_avg_bootstrap, r2r_poli_poli_sim_avg_bootstrap, r2r_ira_ira_sim_avg_bootstrap)\n",
    "print(\"p-value for ANOVA:\", p_val, f_stat)\n",
    "\n",
    "t_stat, p_val = scipy.stats.ttest_ind(r2r_poli_poli_sim_avg_bootstrap, r2r_ira_ira_sim_avg_bootstrap, axis=0, equal_var=False)\n",
    "print(\"p-value for Welch's t-Test between within-Poli and within-IRA:\", p_val, t_stat)\n",
    "\n",
    "t_stat, p_val = scipy.stats.ttest_ind(r2r_ira_rand_sim_avg_bootstrap, r2r_ira_poli_sim_avg_bootstrap, axis=0, equal_var=False)\n",
    "print(\"p-value for Welch's t-Test between IRA-Rand and IRA-Poli:\", p_val, t_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(reddit_user_links_mat_rand.T)\n",
    "plt.show()\n",
    "plt.imshow(reddit_user_links_mat_poli.T)\n",
    "plt.show()\n",
    "plt.imshow(reddit_user_links_mat_ira.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, mat in [\n",
    "    (\"random\", reddit_user_links_mat_rand), \n",
    "    (\"political\", reddit_user_links_mat_poli), \n",
    "    (\"ira\", reddit_user_links_mat_ira)]:\n",
    "    print(label)\n",
    "    domains = top_tld_map_reddit[label]\n",
    "    for tld, prop in sorted(zip(domains, np.mean(mat, axis=0)), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(\"\\t\", tld, prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2t_overlap_rand = top_tld_map_reddit[\"random\"].intersection(top_tld_map_twitter[\"random\"])\n",
    "r2t_overlap_poli = top_tld_map_reddit[\"political\"].intersection(top_tld_map_twitter[\"political\"])\n",
    "r2t_overlap_ira = top_tld_map_reddit[\"ira\"].intersection(top_tld_map_twitter[\"ira\"])\n",
    "\n",
    "# Calculate pairwise similarity among users across platforms\n",
    "# r2t_rand_sim = sklearn.metrics.pairwise.cosine_similarity(reddit_user_links_mat_rand, twitter_user_links_mat_rand)\n",
    "# r2t_poli_sim = sklearn.metrics.pairwise.cosine_similarity(reddit_user_links_mat_poli, twitter_user_links_mat_poli)\n",
    "# r2t_ira_sim = sklearn.metrics.pairwise.cosine_similarity(reddit_user_links_mat_ira, twitter_user_links_mat_ira)\n",
    "\n",
    "r2t_rand_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    links_to_norm_matrix(reddit_links_df_rand, r2t_overlap_rand, \"user_name\"), \n",
    "    links_to_norm_matrix(twitter_links_df_rand, r2t_overlap_rand, \"user_id\"))\n",
    "r2t_poli_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    links_to_norm_matrix(reddit_links_df_poli, r2t_overlap_poli, \"user_name\"), \n",
    "    links_to_norm_matrix(twitter_links_df_poli, r2t_overlap_poli, \"user_id\"))\n",
    "r2t_ira_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    links_to_norm_matrix(reddit_links_df_ira, r2t_overlap_ira, \"user_name\"), \n",
    "    links_to_norm_matrix(twitter_links_df_ira, r2t_overlap_ira, \"user_id\"))\n",
    "\n",
    "# Collapse similarities down to get the mean similarity for each user on the left to all users on the right\n",
    "r2t_rand_sim_avg = np.mean(r2t_rand_sim, axis=1)\n",
    "r2t_poli_sim_avg = np.mean(r2t_poli_sim, axis=1)\n",
    "r2t_ira_sim_avg = np.mean(r2t_ira_sim, axis=1)\n",
    "\n",
    "# Show similarity distribution within populations in this platform\n",
    "r2t_rand_sim_avg_bootstrap = [sklearn.utils.resample(r2t_rand_sim_avg, replace=True, n_samples=r2t_rand_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2t_poli_sim_avg_bootstrap = [sklearn.utils.resample(r2t_poli_sim_avg, replace=True, n_samples=r2t_poli_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2t_ira_sim_avg_bootstrap = [sklearn.utils.resample(r2t_ira_sim_avg, replace=True, n_samples=r2t_ira_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "\n",
    "plt.hist(r2t_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"Random R2T\")\n",
    "plt.hist(r2t_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"Political R2T\")\n",
    "plt.hist(r2t_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"IRA R2T\")\n",
    "\n",
    "plt.title(\"Within-Population, Across-Platform Similarity\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(r2r_ira_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"IRA-Random in Reddit\")\n",
    "plt.hist(r2r_ira_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"IRA-Political in Reddit\")\n",
    "plt.hist(r2t_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"IRA Reddit-to-Twitter\")\n",
    "\n",
    "plt.title(\"Within-Population, Across-Platform Similarity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether the across-platform similarities across the three populations are equal\n",
    "f_stat, p_val = scipy.stats.f_oneway(r2t_rand_sim_avg_bootstrap, r2t_poli_sim_avg_bootstrap, r2t_ira_sim_avg_bootstrap)\n",
    "print(\"p-value for ANOVA:\", p_val, f_stat)\n",
    "\n",
    "t_stat, p_val = scipy.stats.ttest_ind(r2t_rand_sim_avg_bootstrap, r2t_ira_sim_avg_bootstrap, axis=0, equal_var=False)\n",
    "print(\"p-value for Welch's t-Test between Random and IRA:\", p_val, t_stat)\n",
    "\n",
    "t_stat, p_val = scipy.stats.ttest_ind(r2t_poli_sim_avg_bootstrap, r2t_ira_sim_avg_bootstrap, axis=0, equal_var=False)\n",
    "print(\"p-value for Welch's t-Test between Poli and IRA:\", p_val, t_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Channel Distributions\n",
    "\n",
    "We've checked the differences in top-level domain sharing, but we know YouTube was very popular as well. Now, we turn to the distributions of YouTube channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_yt_df_rand = twitter_links_df_rand[twitter_links_df_rand.tld == \"youtube.com\"].copy()\n",
    "twitter_yt_df_poli = twitter_links_df_poli[twitter_links_df_poli.tld == \"youtube.com\"].copy()\n",
    "twitter_yt_df_ira = twitter_links_df_ira[twitter_links_df_ira.tld == \"youtube.com\"].copy()\n",
    "\n",
    "twitter_yt_df_rand[\"video_id\"] = twitter_yt_df_rand.link.apply(strip_video_id_from_url)\n",
    "twitter_yt_df_poli[\"video_id\"] = twitter_yt_df_poli.link.apply(strip_video_id_from_url)\n",
    "twitter_yt_df_ira[\"video_id\"] = twitter_yt_df_ira.link.apply(strip_video_id_from_url)\n",
    "\n",
    "twitter_yt_df_rand = twitter_yt_df_rand.dropna(subset=[\"video_id\"])\n",
    "twitter_yt_df_poli = twitter_yt_df_poli.dropna(subset=[\"video_id\"])\n",
    "twitter_yt_df_ira = twitter_yt_df_ira.dropna(subset=[\"video_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_video_ids = set(twitter_yt_df_rand[\"video_id\"]).union(set(twitter_yt_df_poli[\"video_id\"])).union(set(twitter_yt_df_ira[\"video_id\"]))\n",
    "print(\"Unique YT Videos:\", len(twitter_video_ids))\n",
    "\n",
    "with open(\"twitter_all_video_ids.csv\", \"w\") as out_file:\n",
    "    out_file.write(\"video_id\\n\")\n",
    "    for video_id in twitter_video_ids:\n",
    "        if ( len(video_id.strip()) == 0 ):\n",
    "            continue\n",
    "        out_file.write(\"%s\\n\" % video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_yt_df_rand = reddit_links_df_rand[reddit_links_df_rand.tld == \"youtube.com\"].copy()\n",
    "reddit_yt_df_poli = reddit_links_df_poli[reddit_links_df_poli.tld == \"youtube.com\"].copy()\n",
    "reddit_yt_df_ira = reddit_links_df_ira[reddit_links_df_ira.tld == \"youtube.com\"].copy()\n",
    "\n",
    "reddit_yt_df_rand[\"video_id\"] = reddit_yt_df_rand.link.apply(strip_video_id_from_url)\n",
    "reddit_yt_df_poli[\"video_id\"] = reddit_yt_df_poli.link.apply(strip_video_id_from_url)\n",
    "reddit_yt_df_ira[\"video_id\"] = reddit_yt_df_ira.link.apply(strip_video_id_from_url)\n",
    "\n",
    "reddit_yt_df_rand = reddit_yt_df_rand.dropna(subset=[\"video_id\"])\n",
    "reddit_yt_df_poli = reddit_yt_df_poli.dropna(subset=[\"video_id\"])\n",
    "reddit_yt_df_ira = reddit_yt_df_ira.dropna(subset=[\"video_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_video_ids = set(reddit_yt_df_rand[\"video_id\"]).union(set(reddit_yt_df_poli[\"video_id\"])).union(set(reddit_yt_df_ira[\"video_id\"]))\n",
    "print(\"Unique YT Videos:\", len(reddit_video_ids))\n",
    "\n",
    "with open(\"reddit_all_video_ids.csv\", \"w\") as out_file:\n",
    "    out_file.write(\"video_id\\n\")\n",
    "    for video_id in reddit_video_ids:\n",
    "        if ( len(video_id.strip()) == 0 ):\n",
    "            continue\n",
    "        video_id = video_id.replace(\"%\", \"\")\n",
    "        out_file.write(\"%s\\n\" % video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_video_ids = reddit_video_ids.union(twitter_video_ids)\n",
    "print(\"Merged YT Videos:\", len(all_video_ids))\n",
    "\n",
    "with open(\"twitter+reddit_all_video_ids.csv\", \"w\") as out_file:\n",
    "    out_file.write(\"video_id\\n\")\n",
    "    for video_id in all_video_ids:\n",
    "        if ( len(video_id.strip()) == 0 ):\n",
    "            continue\n",
    "        out_file.write(\"%s\\n\" % video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "twitter_yt_meta_df = pd.read_csv(\"twitter_video_metadata.csv\")\n",
    "twitter_yt_vid2chan_map = {row[\"video_id\"]:row[\"channel_id\"] for idx, row in twitter_yt_meta_df.iterrows()}\n",
    "\n",
    "twitter_yt_df_rand[\"channel_id\"] = twitter_yt_df_rand[\"video_id\"].apply(twitter_yt_vid2chan_map.get)\n",
    "twitter_yt_df_poli[\"channel_id\"] = twitter_yt_df_poli[\"video_id\"].apply(twitter_yt_vid2chan_map.get)\n",
    "twitter_yt_df_ira[\"channel_id\"] = twitter_yt_df_ira[\"video_id\"].apply(twitter_yt_vid2chan_map.get)\n",
    "\n",
    "twitter_top_yt_chans_rand = get_top_channels(twitter_yt_df_rand, user_id_field=\"user_id\", top_n=top_n_channels)\n",
    "twitter_top_yt_chans_poli = get_top_channels(twitter_yt_df_poli, user_id_field=\"user_id\", top_n=top_n_channels)\n",
    "twitter_top_yt_chans_ira = get_top_channels(twitter_yt_df_ira, user_id_field=\"user_id\", top_n=top_n_channels)\n",
    "\n",
    "top_yt_chans_map_twitter = {\n",
    "    \"random\": set(twitter_top_yt_chans_rand),\n",
    "    \"political\": set(twitter_top_yt_chans_poli),\n",
    "    \"ira\": set(twitter_top_yt_chans_ira),\n",
    "}\n",
    "\n",
    "for population, tlds in top_yt_chans_map_twitter.items():\n",
    "    print(population, \":\", len(tlds))\n",
    "\n",
    "# twitter_merged_yt_chans = set(twitter_top_yt_chans_rand)\\\n",
    "#     .intersection(twitter_top_yt_chans_poli)\n",
    "# #     .intersection(twitter_top_yt_chans_ira)\n",
    "# print(\"Merged Top Channels:\", len(twitter_merged_yt_chans))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reddit_yt_meta_df = pd.read_csv(\"reddit_all_video_metadata.csv\")\n",
    "reddit_yt_vid2chan_map = {row[\"video_id\"]:row[\"channel_id\"] for idx, row in reddit_yt_meta_df.iterrows()}\n",
    "\n",
    "reddit_yt_df_rand[\"channel_id\"] = reddit_yt_df_rand[\"video_id\"].apply(reddit_yt_vid2chan_map.get)\n",
    "reddit_yt_df_poli[\"channel_id\"] = reddit_yt_df_poli[\"video_id\"].apply(reddit_yt_vid2chan_map.get)\n",
    "reddit_yt_df_ira[\"channel_id\"] = reddit_yt_df_ira[\"video_id\"].apply(reddit_yt_vid2chan_map.get)\n",
    "\n",
    "reddit_top_yt_chans_rand = get_top_channels(reddit_yt_df_rand, user_id_field=\"user_name\", top_n=top_n_channels)\n",
    "reddit_top_yt_chans_poli = get_top_channels(reddit_yt_df_poli, user_id_field=\"user_name\", top_n=top_n_channels)\n",
    "reddit_top_yt_chans_ira = get_top_channels(reddit_yt_df_ira, user_id_field=\"user_name\", top_n=top_n_channels)\n",
    "\n",
    "top_yt_chans_map_reddit = {\n",
    "    \"random\": set(reddit_top_yt_chans_rand),\n",
    "    \"political\": set(reddit_top_yt_chans_poli),\n",
    "    \"ira\": set(reddit_top_yt_chans_ira),\n",
    "}\n",
    "\n",
    "for population, tlds in top_yt_chans_map_reddit.items():\n",
    "    print(population, \":\", len(tlds))\n",
    "\n",
    "# reddit_merged_yt_chans = set(reddit_top_yt_chans_rand)\\\n",
    "#     .intersection(reddit_top_yt_chans_poli)\n",
    "# #     .intersection(reddit_top_yt_chans_ira)\n",
    "# print(\"Merged Top Channels:\", len(reddit_merged_yt_chans))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_top_channels = reddit_merged_yt_chans.intersection(twitter_merged_yt_chans)\n",
    "# print(\"Cross-Platform Top Channels:\", len(all_top_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels = sorted(all_top_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_overlap_rand_poli = top_yt_chans_map_twitter[\"random\"].intersection(top_yt_chans_map_twitter[\"political\"])\n",
    "twitter_overlap_rand_ira = top_yt_chans_map_twitter[\"random\"].intersection(top_yt_chans_map_twitter[\"ira\"])\n",
    "twitter_overlap_poli_ira = top_yt_chans_map_twitter[\"political\"].intersection(top_yt_chans_map_twitter[\"ira\"])\n",
    "\n",
    "print(\"Rand->Poli Overlap:\", len(twitter_overlap_rand_poli))\n",
    "print(\"Rand->IRA Overlap:\", len(twitter_overlap_rand_ira))\n",
    "print(\"Poli->IRA Overlap:\", len(twitter_overlap_poli_ira))\n",
    "\n",
    "twitter_user_links_mat_rand = channels_to_norm_matrix(twitter_yt_df_rand, top_yt_chans_map_twitter[\"random\"], \"user_id\")\n",
    "twitter_user_links_mat_poli = channels_to_norm_matrix(twitter_yt_df_poli, top_yt_chans_map_twitter[\"political\"], \"user_id\")\n",
    "twitter_user_links_mat_ira = channels_to_norm_matrix(twitter_yt_df_ira, top_yt_chans_map_twitter[\"ira\"], \"user_id\")\n",
    "\n",
    "# Calculate pairwise similarity among users across populations\n",
    "t2t_rand_rand_sim = sklearn.metrics.pairwise.cosine_similarity(twitter_user_links_mat_rand, twitter_user_links_mat_rand)\n",
    "t2t_poli_poli_sim = sklearn.metrics.pairwise.cosine_similarity(twitter_user_links_mat_poli, twitter_user_links_mat_poli)\n",
    "t2t_ira_ira_sim = sklearn.metrics.pairwise.cosine_similarity(twitter_user_links_mat_ira, twitter_user_links_mat_ira)\n",
    "\n",
    "t2t_ira_rand_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    channels_to_norm_matrix(twitter_yt_df_ira, twitter_overlap_rand_ira, \"user_id\"), \n",
    "    channels_to_norm_matrix(twitter_yt_df_rand, twitter_overlap_rand_ira, \"user_id\"))\n",
    "t2t_ira_poli_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    channels_to_norm_matrix(twitter_yt_df_ira, twitter_overlap_poli_ira, \"user_id\"), \n",
    "    channels_to_norm_matrix(twitter_yt_df_poli, twitter_overlap_poli_ira, \"user_id\"))\n",
    "t2t_rand_poli_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    channels_to_norm_matrix(twitter_yt_df_rand, twitter_overlap_rand_poli, \"user_id\"), \n",
    "    channels_to_norm_matrix(twitter_yt_df_poli, twitter_overlap_rand_poli, \"user_id\"))\n",
    "\n",
    "# Collapse similarities down to get the mean similarity for each user on the left to all users on the right\n",
    "#. note the minus 1 and reduction of shape by 1 for the within-platform groups, which I do to remove the \n",
    "#. self-similarity effect\n",
    "t2t_rand_rand_sim_avg = (np.sum(t2t_rand_rand_sim, axis=1) - 1) / (twitter_user_links_mat_rand.shape[0] - 1)\n",
    "t2t_poli_poli_sim_avg = (np.sum(t2t_poli_poli_sim, axis=1) - 1) / (twitter_user_links_mat_poli.shape[0] - 1)\n",
    "t2t_ira_ira_sim_avg = (np.sum(t2t_ira_ira_sim, axis=1) - 1) / (twitter_user_links_mat_ira.shape[0] - 1)\n",
    "t2t_ira_rand_sim_avg = np.mean(t2t_ira_rand_sim, axis=1)\n",
    "t2t_ira_poli_sim_avg = np.mean(t2t_ira_poli_sim, axis=1)\n",
    "t2t_ira_ira_sim_avg = np.mean(t2t_ira_ira_sim, axis=1)\n",
    "t2t_rand_poli_sim_avg = np.sum(t2t_rand_poli_sim, axis=1)\n",
    "\n",
    "# # Plot what these similarities look like\n",
    "# plt.hist(t2t_ira_rand_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-rand\")\n",
    "# plt.hist(t2t_ira_poli_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-poli\")\n",
    "# plt.hist(t2t_ira_ira_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Show similarity distribution within populations in this platform\n",
    "t2t_rand_rand_sim_avg_bootstrap = [sklearn.utils.resample(t2t_rand_rand_sim_avg, replace=True, n_samples=t2t_rand_rand_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "t2t_poli_poli_sim_avg_bootstrap = [sklearn.utils.resample(t2t_poli_poli_sim_avg, replace=True, n_samples=t2t_poli_poli_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "t2t_ira_ira_sim_avg_bootstrap = [sklearn.utils.resample(t2t_ira_ira_sim_avg, replace=True, n_samples=t2t_ira_ira_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "\n",
    "plt.hist(t2t_rand_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"rand-rand\")\n",
    "plt.hist(t2t_poli_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"poli-poli\")\n",
    "plt.hist(t2t_ira_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "plt.title(\"Within-Population Similarity\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Show similarity distribution across populations in this platform\n",
    "t2t_ira_rand_sim_avg_bootstrap = [sklearn.utils.resample(t2t_ira_rand_sim_avg, replace=True, n_samples=t2t_ira_rand_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "t2t_ira_poli_sim_avg_bootstrap = [sklearn.utils.resample(t2t_ira_poli_sim_avg, replace=True, n_samples=t2t_ira_poli_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "t2t_ira_ira_sim_avg_bootstrap = [sklearn.utils.resample(t2t_ira_ira_sim_avg, replace=True, n_samples=t2t_ira_ira_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "\n",
    "plt.hist(t2t_ira_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-rand\")\n",
    "plt.hist(t2t_ira_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-poli\")\n",
    "plt.hist(t2t_ira_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "plt.title(\"Across-Population Similarity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_overlap_rand_poli = top_yt_chans_map_reddit[\"random\"].intersection(top_yt_chans_map_reddit[\"political\"])\n",
    "reddit_overlap_rand_ira = top_yt_chans_map_reddit[\"random\"].intersection(top_yt_chans_map_reddit[\"ira\"])\n",
    "reddit_overlap_poli_ira = top_yt_chans_map_reddit[\"political\"].intersection(top_yt_chans_map_reddit[\"ira\"])\n",
    "\n",
    "print(\"Rand->Poli Overlap:\", len(reddit_overlap_rand_poli))\n",
    "print(\"Rand->IRA Overlap:\", len(reddit_overlap_rand_ira))\n",
    "print(\"Poli->IRA Overlap:\", len(reddit_overlap_poli_ira))\n",
    "\n",
    "reddit_user_links_mat_rand = channels_to_norm_matrix(reddit_yt_df_rand, top_yt_chans_map_twitter[\"random\"], \"user_name\")\n",
    "reddit_user_links_mat_poli = channels_to_norm_matrix(reddit_yt_df_poli, top_yt_chans_map_twitter[\"political\"], \"user_name\")\n",
    "reddit_user_links_mat_ira = channels_to_norm_matrix(reddit_yt_df_ira, top_yt_chans_map_twitter[\"ira\"], \"user_name\")\n",
    "\n",
    "# Calculate pairwise similarity among users across populations\n",
    "r2r_rand_rand_sim = sklearn.metrics.pairwise.cosine_similarity(reddit_user_links_mat_rand, reddit_user_links_mat_rand)\n",
    "r2r_poli_poli_sim = sklearn.metrics.pairwise.cosine_similarity(reddit_user_links_mat_poli, reddit_user_links_mat_poli)\n",
    "r2r_ira_ira_sim = sklearn.metrics.pairwise.cosine_similarity(reddit_user_links_mat_ira, reddit_user_links_mat_ira)\n",
    "\n",
    "r2r_ira_rand_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    channels_to_norm_matrix(reddit_yt_df_ira, reddit_overlap_rand_ira, \"user_name\"), \n",
    "    channels_to_norm_matrix(reddit_yt_df_rand, reddit_overlap_rand_ira, \"user_name\"))\n",
    "r2r_ira_poli_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    channels_to_norm_matrix(reddit_yt_df_ira, reddit_overlap_poli_ira, \"user_name\"), \n",
    "    channels_to_norm_matrix(reddit_yt_df_poli, reddit_overlap_poli_ira, \"user_name\"))\n",
    "r2r_rand_poli_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    channels_to_norm_matrix(reddit_yt_df_rand, reddit_overlap_rand_poli, \"user_name\"), \n",
    "    channels_to_norm_matrix(reddit_yt_df_poli, reddit_overlap_rand_poli, \"user_name\"))\n",
    "\n",
    "# Collapse similarities down to get the mean similarity for each user on the left to all users on the right\n",
    "#. note the minus 1 and reduction of shape by 1 for the within-platform groups, which I do to remove the \n",
    "#. self-similarity effect\n",
    "r2r_rand_rand_sim_avg = (np.sum(r2r_rand_rand_sim, axis=1) - 1) / (reddit_user_links_mat_rand.shape[0] - 1)\n",
    "r2r_poli_poli_sim_avg = (np.sum(r2r_poli_poli_sim, axis=1) - 1) / (reddit_user_links_mat_poli.shape[0] - 1)\n",
    "r2r_ira_ira_sim_avg = (np.sum(r2r_ira_ira_sim, axis=1) - 1) / (reddit_user_links_mat_ira.shape[0] - 1)\n",
    "r2r_ira_rand_sim_avg = np.mean(r2r_ira_rand_sim, axis=1)\n",
    "r2r_ira_poli_sim_avg = np.mean(r2r_ira_poli_sim, axis=1)\n",
    "r2r_ira_ira_sim_avg = np.mean(r2r_ira_ira_sim, axis=1)\n",
    "r2r_rand_poli_sim_avg = np.sum(r2r_rand_poli_sim, axis=1)\n",
    "\n",
    "# # Plot what these similarities look like\n",
    "# plt.hist(r2r_ira_rand_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-rand\")\n",
    "# plt.hist(r2r_ira_poli_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-poli\")\n",
    "# plt.hist(r2r_ira_ira_sim_avg, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Show similarity distribution within populations in this platform\n",
    "r2r_rand_rand_sim_avg_bootstrap = [sklearn.utils.resample(r2r_rand_rand_sim_avg, replace=True, n_samples=r2r_rand_rand_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2r_poli_poli_sim_avg_bootstrap = [sklearn.utils.resample(r2r_poli_poli_sim_avg, replace=True, n_samples=r2r_poli_poli_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2r_ira_ira_sim_avg_bootstrap = [sklearn.utils.resample(r2r_ira_ira_sim_avg, replace=True, n_samples=r2r_ira_ira_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "\n",
    "plt.hist(r2r_rand_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"rand-rand\")\n",
    "plt.hist(r2r_poli_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"poli-poli\")\n",
    "plt.hist(r2r_ira_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "plt.title(\"Within-Population Similarity\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Show similarity distribution across populations in this platform\n",
    "r2r_ira_rand_sim_avg_bootstrap = [sklearn.utils.resample(r2r_ira_rand_sim_avg, replace=True, n_samples=r2r_ira_rand_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2r_ira_poli_sim_avg_bootstrap = [sklearn.utils.resample(r2r_ira_poli_sim_avg, replace=True, n_samples=r2r_ira_poli_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2r_ira_ira_sim_avg_bootstrap = [sklearn.utils.resample(r2r_ira_ira_sim_avg, replace=True, n_samples=r2r_ira_ira_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "\n",
    "plt.hist(r2r_ira_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-rand\")\n",
    "plt.hist(r2r_ira_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-poli\")\n",
    "plt.hist(r2r_ira_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"ira-ira\")\n",
    "\n",
    "plt.title(\"Across-Population Similarity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2t_overlap_rand = top_yt_chans_map_reddit[\"random\"].intersection(top_yt_chans_map_twitter[\"random\"])\n",
    "r2t_overlap_poli = top_yt_chans_map_reddit[\"political\"].intersection(top_yt_chans_map_twitter[\"political\"])\n",
    "r2t_overlap_ira = top_yt_chans_map_reddit[\"ira\"].intersection(top_yt_chans_map_twitter[\"ira\"])\n",
    "\n",
    "print(\"Random Overlap:\", len(r2t_overlap_rand))\n",
    "print(\"Political Overlap:\", len(r2t_overlap_poli))\n",
    "print(\"IRA Overlap:\", len(r2t_overlap_ira))\n",
    "\n",
    "# Calculate pairwise similarity among users across platforms\n",
    "r2t_rand_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    channels_to_norm_matrix(reddit_yt_df_rand, r2t_overlap_rand, \"user_name\"), \n",
    "    channels_to_norm_matrix(twitter_yt_df_rand, r2t_overlap_rand, \"user_id\"))\n",
    "r2t_poli_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    channels_to_norm_matrix(reddit_yt_df_poli, r2t_overlap_poli, \"user_name\"), \n",
    "    channels_to_norm_matrix(twitter_yt_df_poli, r2t_overlap_poli, \"user_id\"))\n",
    "r2t_ira_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    channels_to_norm_matrix(reddit_yt_df_ira, r2t_overlap_ira, \"user_name\"), \n",
    "    channels_to_norm_matrix(twitter_yt_df_ira, r2t_overlap_ira, \"user_id\"))\n",
    "\n",
    "# Collapse similarities down to get the mean similarity for each user on the left to all users on the right\n",
    "r2t_rand_sim_avg = np.mean(r2t_rand_sim, axis=1)\n",
    "r2t_poli_sim_avg = np.mean(r2t_poli_sim, axis=1)\n",
    "r2t_ira_sim_avg = np.mean(r2t_ira_sim, axis=1)\n",
    "\n",
    "# Show similarity distribution within populations in this platform\n",
    "r2t_rand_sim_avg_bootstrap = [sklearn.utils.resample(r2t_rand_sim_avg, replace=True, n_samples=r2t_rand_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2t_poli_sim_avg_bootstrap = [sklearn.utils.resample(r2t_poli_sim_avg, replace=True, n_samples=r2t_poli_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "r2t_ira_sim_avg_bootstrap = [sklearn.utils.resample(r2t_ira_sim_avg, replace=True, n_samples=r2t_ira_sim_avg.shape[0]).mean() for i in range(bootstrap_count)]\n",
    "\n",
    "plt.hist(r2t_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"Random R2T\")\n",
    "plt.hist(r2t_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"Political R2T\")\n",
    "plt.hist(r2t_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"IRA R2T\")\n",
    "\n",
    "plt.title(\"Within-Population, Across-Platform Similarity\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(r2r_ira_rand_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"IRA-Random in Reddit\")\n",
    "plt.hist(r2r_ira_poli_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"IRA-Political in Reddit\")\n",
    "plt.hist(r2t_ira_sim_avg_bootstrap, bins=20, density=True, alpha=0.35, label=\"IRA Reddit-to-Twitter\")\n",
    "\n",
    "plt.title(\"Within-Population, Across-Platform Similarity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "channel_map_reddit = {row[\"channel_id\"]:row[\"channel_title\"] for _, row in reddit_yt_meta_df.iterrows()}\n",
    "channel_map_twitter = {row[\"channel_id\"]:row[\"channel_title\"] for _, row in twitter_yt_meta_df.iterrows()}\n",
    "\n",
    "print(len(top_yt_chans_map_reddit[\"ira\"]), len(top_yt_chans_map_twitter[\"ira\"]))\n",
    "print(\"*\"*100, \"\\nReddit:\")\n",
    "for x in sorted(top_yt_chans_map_reddit[\"ira\"]):\n",
    "    print(x, channel_map_reddit[x])\n",
    "    \n",
    "print(\"*\"*100, \"\\nTwitter:\")\n",
    "for x in sorted(top_yt_chans_map_twitter[\"ira\"]):\n",
    "    print(x, channel_map_twitter[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
